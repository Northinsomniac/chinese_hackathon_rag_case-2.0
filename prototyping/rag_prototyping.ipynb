{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb39041-134e-4b57-9fe7-0bf6d1224b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:04.419399Z",
     "iopub.status.busy": "2024-06-15T03:54:04.418574Z",
     "iopub.status.idle": "2024-06-15T03:54:09.567481Z",
     "shell.execute_reply": "2024-06-15T03:54:09.566786Z",
     "shell.execute_reply.started": "2024-06-15T03:54:04.419362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests==2.28.1 in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: opensearch-py in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.6.0)\n",
      "Requirement already satisfied: pdf2image in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from -r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: langchain in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.2.4)\n",
      "Requirement already satisfied: sentence_transformers in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: InstructorEmbedding in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: unstructured in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.14.5)\n",
      "Requirement already satisfied: langchain-community in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.20.24)\n",
      "Requirement already satisfied: fastwarc in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.14.7)\n",
      "Requirement already satisfied: langchain_huggingface in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.0.3)\n",
      "Requirement already satisfied: html2text in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2024.2.26)\n",
      "Requirement already satisfied: pyspark in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.5.1)\n",
      "Requirement already satisfied: spark-nlp in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (5.3.3)\n",
      "Requirement already satisfied: zhon in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.0.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.8.1)\n",
      "Requirement already satisfied: bs4 in /home/jupyter/.local/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jupyter/.local/lib/python3.10/site-packages (from requests==2.28.1->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests==2.28.1->-r requirements.txt (line 1)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from opensearch-py->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from opensearch-py->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: Events in /home/jupyter/.local/lib/python3.10/site-packages (from opensearch-py->-r requirements.txt (line 2)) (0.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image->-r requirements.txt (line 3)) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.6 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 5)) (0.2.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 5)) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 5)) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (1.22.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (1.10.12)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (8.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (2.0.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.23.3)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: filetype in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (0.4.27)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (4.9.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (4.11.2)\n",
      "Requirement already satisfied: emoji in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (2.12.1)\n",
      "Requirement already satisfied: dataclasses-json in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (3.9.3)\n",
      "Requirement already satisfied: backoff in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (4.7.1)\n",
      "Requirement already satisfied: unstructured-client in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 8)) (0.8.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured->-r requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.24 in /usr/local/lib/python3.10/dist-packages (from boto3->-r requirements.txt (line 10)) (1.23.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->-r requirements.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from boto3->-r requirements.txt (line 10)) (0.5.2)\n",
      "Requirement already satisfied: brotli in /home/jupyter/.local/lib/python3.10/site-packages (from fastwarc->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from fastwarc->-r requirements.txt (line 11)) (8.1.6)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_huggingface->-r requirements.txt (line 12)) (0.19.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->-r requirements.txt (line 14)) (0.10.9.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 17)) (2022.10.31)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json->unstructured->-r requirements.txt (line 8)) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json->unstructured->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r requirements.txt (line 6)) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r requirements.txt (line 6)) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.6->langchain->-r requirements.txt (line 5)) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 5)) (3.10.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (16.0.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers->-r requirements.txt (line 6)) (0.4.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured->-r requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured-client->unstructured->-r requirements.txt (line 8)) (1.0.6)\n",
      "Requirement already satisfied: marshmallow-enum>=1.5.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured-client->unstructured->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from unstructured-client->unstructured->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jupyter/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.6->langchain->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt # установка зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468eb14-e948-47ba-a202-75afffa12959",
   "metadata": {},
   "source": [
    "# OpenSearch connection stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959b6b6-db93-4128-8199-5022a5d526d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.569453Z",
     "iopub.status.busy": "2024-06-15T03:54:09.568860Z",
     "iopub.status.idle": "2024-06-15T03:54:09.603994Z",
     "shell.execute_reply": "2024-06-15T03:54:09.603269Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.569416Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-06-15 03:54:09--  https://storage.yandexcloud.net/cloud-certs/CA.pem\n",
      "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
      "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3579 (3.5K) [application/x-x509-ca-cert]\n",
      "Saving to: ‘/home/jupyter/datasphere/project/.opensearch/root.crt’\n",
      "\n",
      "     0K ...                                                   100% 4.11G=0s\n",
      "\n",
      "2024-06-15 03:54:09 (4.11 GB/s) - ‘/home/jupyter/datasphere/project/.opensearch/root.crt’ saved [3579/3579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# все в доке\n",
    "# сертификат для общение\n",
    "!mkdir -p /home/jupyter/datasphere/project/.opensearch && \\\n",
    "wget \"https://storage.yandexcloud.net/cloud-certs/CA.pem\" \\\n",
    "     --output-document /home/jupyter/datasphere/project/.opensearch/root.crt && \\\n",
    "chmod 0600 /home/jupyter/datasphere/project/.opensearch/root.crt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb52804e-6723-4283-866e-bea751dc9c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.605867Z",
     "iopub.status.busy": "2024-06-15T03:54:09.605200Z",
     "iopub.status.idle": "2024-06-15T03:54:09.746575Z",
     "shell.execute_reply": "2024-06-15T03:54:09.745799Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.605842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'rc1a-lhl99getqermp0v5.mdb.yandexcloud.net', 'cluster_name': 'c9q15tn5hintqrll89ho', 'cluster_uuid': 'F_9hcBS6S4q88k4QTSU0xA', 'version': {'distribution': 'opensearch', 'number': '2.8.0', 'build_type': 'deb', 'build_hash': '8cbab9609696cd93c45fd5f3090560648c04f5af', 'build_date': '2023-10-04T14:42:52.695597332Z', 'build_snapshot': False, 'lucene_version': '9.6.0', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    }
   ],
   "source": [
    "# хранилище наших эмбедингов и подготовленных статей\n",
    "# копирование с доки яндекса\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "CA = '/home/jupyter/datasphere/project/.opensearch/root.crt'\n",
    "\n",
    "PASS = ''\n",
    "HOSTS = [\n",
    "  \"rc1a-lhl99getqermp0v5.mdb.yandexcloud.net\"\n",
    "]\n",
    "\n",
    "conn = OpenSearch(\n",
    "  HOSTS,\n",
    "  http_auth=('admin', PASS),\n",
    "  use_ssl=True,\n",
    "  verify_certs=True,\n",
    "  ca_certs=CA)\n",
    "\n",
    "print(conn.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558399c5-d582-4140-94ac-1e005554a6a8",
   "metadata": {},
   "source": [
    "# Yandex GPT \n",
    "## Getting IAM Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9956f4ed-1908-432a-a306-b1a49970f7e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.749160Z",
     "iopub.status.busy": "2024-06-15T03:54:09.748315Z",
     "iopub.status.idle": "2024-06-15T03:54:09.775707Z",
     "shell.execute_reply": "2024-06-15T03:54:09.775002Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.749104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import jwt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953884e5-bde1-4d2d-90b6-ad2df8c8b958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.777680Z",
     "iopub.status.busy": "2024-06-15T03:54:09.776671Z",
     "iopub.status.idle": "2024-06-15T03:54:09.789009Z",
     "shell.execute_reply": "2024-06-15T03:54:09.788427Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.777640Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_account_id = \"ajeg5vo84hl3nu5a6o98\"\n",
    "key_id = \"aje9l25he6merc8n2o8g\"\n",
    "private_key = \"\"\"\n",
    "PLEASE DO NOT REMOVE THIS LINE! Yandex.Cloud SA Key ID <aje9l25he6merc8n2o8g>\n",
    "-----BEGIN PRIVATE KEY-----\n",
    "...\n",
    "-----END PRIVATE KEY-----\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9801115-f2c8-4744-8843-87c63dd8d5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.790540Z",
     "iopub.status.busy": "2024-06-15T03:54:09.789893Z",
     "iopub.status.idle": "2024-06-15T03:54:09.943045Z",
     "shell.execute_reply": "2024-06-15T03:54:09.942267Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.790506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJ0eXAiOiJKV1QiLCJhbGciOiJQUzI1NiIsImtpZCI6ImFqZTlsMjVoZTZtZXJjOG4ybzhnIn0.eyJhdWQiOiJodHRwczovL2lhbS5hcGkuY2xvdWQueWFuZGV4Lm5ldC9pYW0vdjEvdG9rZW5zIiwiaXNzIjoiYWplZzV2bzg0aGwzbnU1YTZvOTgiLCJpYXQiOjE3MTg0MjM2NDksImV4cCI6MTcxODQyNzI0OX0.IEu1LQ4dDTtI0F6Uw9YUzQzH7hgFlAcweLvUVOOPf28awfZ7jJp33wmz4K-_Bm7dQhjc_sghS0TtAQPXK_-eZ2SYa1jyoNx9dN83PZhv2m4nZau6k8Kgquo29mL-ZII5YDDsVcuLBotffp4stfE88osOffzlVSQMSt3NpR2-TyIjaukaLWfR6xdllHUwfT0RSW9WrkXnj0D1wyAC9hTcmMcphAUaGjai1cpb9CQbYzwgMm-dPVjfbZ6eIyrlXyax2KeUIH7ZUo4cZqAHvcisB34H0OxOYj1sqkavIuTcl790YJgrdoDpou8NqY_ICkA5ZxQEKrEBFfWb8x7Nm8s9zA\n"
     ]
    }
   ],
   "source": [
    "# Получаем IAM-токен\n",
    "\n",
    "now = int(time.time())\n",
    "payload = {\n",
    "        'aud': 'https://iam.api.cloud.yandex.net/iam/v1/tokens',\n",
    "        'iss': service_account_id,\n",
    "        'iat': now,\n",
    "        'exp': now + 3600\n",
    "      }\n",
    "\n",
    "\n",
    "# Формирование JWT.\n",
    "encoded_token = jwt.encode(\n",
    "    payload,\n",
    "    private_key,\n",
    "    algorithm='PS256',\n",
    "    headers={'kid': key_id}\n",
    "  )\n",
    "\n",
    "\n",
    "#Запись ключа в файл\n",
    "with open('jwt_token.txt', 'w') as j:\n",
    "   j.write(encoded_token) \n",
    "   \n",
    "# Вывод в консоль\n",
    "print(encoded_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17ad58-c7c6-4f22-a8ad-d0ed8aef1ad4",
   "metadata": {},
   "source": [
    "# Data Pipeline Started Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354b08ec-691f-46af-bab2-725de29acad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:09.944738Z",
     "iopub.status.busy": "2024-06-15T03:54:09.944048Z",
     "iopub.status.idle": "2024-06-15T03:54:10.249334Z",
     "shell.execute_reply": "2024-06-15T03:54:10.248672Z",
     "shell.execute_reply.started": "2024-06-15T03:54:09.944703Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import LLMChain\n",
    "from YaGPT import YandexGPTEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a89fc36-cd7e-4eca-b984-f8139399e7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.251095Z",
     "iopub.status.busy": "2024-06-15T03:54:10.250278Z",
     "iopub.status.idle": "2024-06-15T03:54:10.260431Z",
     "shell.execute_reply": "2024-06-15T03:54:10.259762Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.251060Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dir = \"china-warc-archieves/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab5978-fb8a-48fd-a881-d082409e3431",
   "metadata": {},
   "source": [
    "# Load Data warc.gz files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0f2ea4-9706-42c2-ba17-4ac610ebeec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.262723Z",
     "iopub.status.busy": "2024-06-15T03:54:10.261305Z",
     "iopub.status.idle": "2024-06-15T03:54:10.308666Z",
     "shell.execute_reply": "2024-06-15T03:54:10.308078Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.262686Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "from pathlib import Path\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de74ffee-9643-42dc-bac3-77e367d70910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.311350Z",
     "iopub.status.busy": "2024-06-15T03:54:10.310678Z",
     "iopub.status.idle": "2024-06-15T03:54:10.321849Z",
     "shell.execute_reply": "2024-06-15T03:54:10.321269Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.311321Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ключи доступа к бакету\n",
    "access_id = \"YCAJEey3P4Gm2ATCnQWb5Auh-\"\n",
    "access_secret = \"YCOKQjkkP15RdJ9Mh-dwWju1zTo-fgYrxm_NBtJ5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c9f94a-f805-4054-aba0-7c69f2f3912c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.323498Z",
     "iopub.status.busy": "2024-06-15T03:54:10.322768Z",
     "iopub.status.idle": "2024-06-15T03:54:10.378255Z",
     "shell.execute_reply": "2024-06-15T03:54:10.377643Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.323466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url='https://storage.yandexcloud.net',\n",
    "    aws_access_key_id=access_id,\n",
    "    aws_secret_access_key=access_secret,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35dbd3f5-b176-459b-a08d-3591563b5745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.380223Z",
     "iopub.status.busy": "2024-06-15T03:54:10.379285Z",
     "iopub.status.idle": "2024-06-15T03:54:10.507916Z",
     "shell.execute_reply": "2024-06-15T03:54:10.507204Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.380193Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cn.reuters.com-2017-01.warc.gz', 'cn.reuters.com-2017-02.warc.gz', 'cn.reuters.com-2017-03.warc.gz', 'cn.reuters.com-2017-04.warc.gz', 'finance.people.com.cn-2019-08.warc.gz', 'finance.people.com.cn-2019-09.warc.gz', 'finance.people.com.cn-2022-04.warc.gz', 'finance.people.com.cn-2022-05.warc.gz', 'finance.people.com.cn-2023-04.warc.gz', 'finance.people.com.cn-2023-06.warc.gz', 'finance.people.com.cn-2023-07.warc.gz', 'health.people.com.cn-2019-08.warc.gz', 'health.people.com.cn-2019-09.warc.gz', 'health.people.com.cn-2023-04.warc.gz', 'health.people.com.cn-2023-05.warc.gz', 'health.people.com.cn-2023-07.warc.gz', 'inews.hket.com-2022-04.warc.gz', 'inews.hket.com-2022-05.warc.gz', 'inews.hket.com-2022-06.warc.gz', 'inews.hket.com-2023-03.warc.gz', 'inews.hket.com-2023-06.warc.gz', 'inews.hket.com-2023-07.warc.gz', 'news.ltn.com.tw-2016-08.warc.gz', 'news.ltn.com.tw-2016-09.warc.gz', 'news.ltn.com.tw-2016-12.warc.gz', 'news.ltn.com.tw-2017-02.warc.gz', 'news.ltn.com.tw-2017-03.warc.gz', 'news.ltn.com.tw-2017-04.warc.gz', 'news.ltn.com.tw-2022-06.warc.gz', 'news.ltn.com.tw-2023-04.warc.gz', 'news.ltn.com.tw-2023-06.warc.gz', 'news.ltn.com.tw-2023-07.warc.gz', 'news.mingpao.com-2019-09.warc.gz', 'news.mingpao.com-2022-04.warc.gz', 'news.mingpao.com-2022-06.warc.gz', 'news.mingpao.com-2023-04.warc.gz', 'news.mingpao.com-2023-06.warc.gz', 'news.mingpao.com-2023-07.warc.gz', 'news.sina.com.tw-2016-12.warc.gz', 'news.sina.com.tw-2017-01.warc.gz', 'news.sina.com.tw-2017-02.warc.gz', 'news.sina.com.tw-2017-03.warc.gz', 'news.sina.com.tw-2017-04.warc.gz', 'orientaldaily.on.cc-2023-03.warc.gz', 'orientaldaily.on.cc-2023-04.warc.gz', 'topick.hket.com-2022-04.warc.gz', 'topick.hket.com-2023-03.warc.gz', 'topick.hket.com-2023-04.warc.gz', 'topick.hket.com-2023-06.warc.gz', 'world.people.com.cn-2019-08.warc.gz', 'world.people.com.cn-2022-05.warc.gz', 'world.people.com.cn-2022-06.warc.gz', 'world.people.com.cn-2023-03.warc.gz', 'world.people.com.cn-2023-04.warc.gz', 'world.people.com.cn-2023-05.warc.gz', 'world.people.com.cn-2023-06.warc.gz', 'world.people.com.cn-2023-07.warc.gz', 'www.am730.com.hk-2022-06.warc.gz', 'www.am730.com.hk-2023-04.warc.gz', 'www.am730.com.hk-2023-05.warc.gz', 'www.am730.com.hk-2023-06.warc.gz', 'www.am730.com.hk-2023-07.warc.gz', 'www.chinatimes.com-2016-08.warc.gz', 'www.chinatimes.com-2016-09.warc.gz', 'www.chinatimes.com-2016-12.warc.gz', 'www.chinatimes.com-2017-01.warc.gz', 'www.chinatimes.com-2017-02.warc.gz', 'www.chinatimes.com-2017-03.warc.gz', 'www.chinatimes.com-2017-04.warc.gz', 'www.chinatimes.com-2019-08.warc.gz', 'www.chinatimes.com-2019-09.warc.gz', 'www.chinatimes.com-2022-04.warc.gz', 'www.chinatimes.com-2022-05.warc.gz', 'www.chinatimes.com-2022-06.warc.gz', 'www.chinatimes.com-2023-03.warc.gz', 'www.chinatimes.com-2023-04.warc.gz', 'www.chinatimes.com-2023-05.warc.gz', 'www.chinatimes.com-2023-06.warc.gz', 'www.chinatimes.com-2023-07.warc.gz', 'www.cna.com.tw-2017-01.warc.gz', 'www.cna.com.tw-2017-02.warc.gz', 'www.cna.com.tw-2019-08.warc.gz', 'www.cna.com.tw-2019-09.warc.gz', 'www.cna.com.tw-2022-04.warc.gz', 'www.cna.com.tw-2022-05.warc.gz', 'www.cna.com.tw-2022-06.warc.gz', 'www.cna.com.tw-2023-03.warc.gz', 'www.cna.com.tw-2023-04.warc.gz', 'www.cna.com.tw-2023-07.warc.gz', 'www.etnet.com.hk-2019-08.warc.gz', 'www.etnet.com.hk-2019-09.warc.gz', 'www.etnet.com.hk-2022-05.warc.gz', 'www.etnet.com.hk-2022-06.warc.gz', 'www.etnet.com.hk-2023-03.warc.gz', 'www.etnet.com.hk-2023-04.warc.gz', 'www.etnet.com.hk-2023-05.warc.gz', 'www.etnet.com.hk-2023-06.warc.gz', 'www.etnet.com.hk-2023-07.warc.gz', 'www.hk01.com-2019-08.warc.gz', 'www.hk01.com-2019-09.warc.gz', 'www.hk01.com-2022-05.warc.gz', 'www.hk01.com-2022-06.warc.gz', 'www.hk01.com-2023-04.warc.gz', 'www.hk01.com-2023-05.warc.gz', 'www.hk01.com-2023-06.warc.gz', 'www.hk01.com-2023-07.warc.gz', 'www.inform.kz-2023-03.warc.gz', 'www.inform.kz-2023-04.warc.gz', 'www.inform.kz-2023-05.warc.gz', 'www.inform.kz-2023-06.warc.gz', 'www.inform.kz-2023-07.warc.gz', 'www.ithome.com-2019-08.warc.gz', 'www.ithome.com-2019-09.warc.gz', 'www.ithome.com-2022-04.warc.gz', 'www.ithome.com-2022-06.warc.gz', 'www.ithome.com-2023-05.warc.gz', 'www.nownews.com-2019-09.warc.gz', 'www.nownews.com-2022-04.warc.gz', 'www.nownews.com-2022-05.warc.gz', 'www.nownews.com-2022-06.warc.gz', 'www.nownews.com-2023-03.warc.gz', 'www.nownews.com-2023-04.warc.gz', 'www.nownews.com-2023-06.warc.gz', 'www.rfi.fr-2022-04.warc.gz', 'www.rfi.fr-2022-05.warc.gz', 'www.rfi.fr-2022-06.warc.gz', 'www.rfi.fr-2023-03.warc.gz', 'www.rfi.fr-2023-04.warc.gz', 'www.rfi.fr-2023-05.warc.gz', 'www.rfi.fr-2023-06.warc.gz', 'www.rfi.fr-2023-07.warc.gz', 'www.taiwannews.com.tw-2017-01.warc.gz', 'www.taiwannews.com.tw-2017-02.warc.gz', 'www.taiwannews.com.tw-2017-04.warc.gz', 'www.taiwannews.com.tw-2019-08.warc.gz', 'www.taiwannews.com.tw-2019-09.warc.gz', 'www.taiwannews.com.tw-2022-05.warc.gz', 'www.taiwannews.com.tw-2023-03.warc.gz', 'www.thenewslens.com-2019-08.warc.gz', 'www.thenewslens.com-2022-04.warc.gz', 'www.zaobao.com.sg-2017-04.warc.gz', 'www.zaobao.com.sg-2019-09.warc.gz', 'www.zaobao.com.sg-2022-04.warc.gz', 'www.zaobao.com.sg-2022-05.warc.gz', 'www.zaobao.com.sg-2023-03.warc.gz', 'www.zaobao.com.sg-2023-04.warc.gz', 'www.zaobao.com.sg-2023-05.warc.gz', 'www.zaobao.com.sg-2023-06.warc.gz', 'www.zaobao.com.sg-2023-07.warc.gz', 'www2.hkej.com-2016-12.warc.gz', 'www2.hkej.com-2017-01.warc.gz', 'www2.hkej.com-2017-03.warc.gz', 'www2.hkej.com-2017-04.warc.gz', 'www2.hkej.com-2019-08.warc.gz']\n"
     ]
    }
   ],
   "source": [
    "# Specify the bucket name and prefix if needed\n",
    "bucket_name = 'china-warc-archieves'\n",
    "prefix = ''\n",
    "\n",
    "def list_files(bucket_name, prefix, s3_client):\n",
    "    for key in s3.list_objects(Bucket=bucket_name)['Contents']:\n",
    "        if key['Key'].endswith('.warc.gz'):\n",
    "            yield key['Key']\n",
    "        continue\n",
    "\n",
    "files = list(list_files(bucket_name, prefix, s3))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a28a9-2b5f-4673-9a6a-d9aeeb3c6fee",
   "metadata": {},
   "source": [
    "## Find csv file with already parsed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef6cf28b-d20f-430f-b20a-5639452dd202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.509654Z",
     "iopub.status.busy": "2024-06-15T03:54:10.508827Z",
     "iopub.status.idle": "2024-06-15T03:54:10.535309Z",
     "shell.execute_reply": "2024-06-15T03:54:10.534708Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.509622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cn.reuters.com-2017-01.warc.gz',\n",
       " 'cn.reuters.com-2017-02.warc.gz',\n",
       " 'cn.reuters.com-2017-03.warc.gz',\n",
       " 'cn.reuters.com-2017-04.warc.gz',\n",
       " 'finance.people.com.cn-2019-08.warc.gz',\n",
       " 'finance.people.com.cn-2022-04.warc.gz',\n",
       " 'finance.people.com.cn-2022-05.warc.gz',\n",
       " 'finance.people.com.cn-2023-04.warc.gz',\n",
       " 'finance.people.com.cn-2023-06.warc.gz',\n",
       " 'finance.people.com.cn-2023-07.warc.gz',\n",
       " 'health.people.com.cn-2019-08.warc.gz',\n",
       " 'health.people.com.cn-2019-09.warc.gz',\n",
       " 'health.people.com.cn-2023-04.warc.gz',\n",
       " 'health.people.com.cn-2023-05.warc.gz',\n",
       " 'health.people.com.cn-2023-07.warc.gz',\n",
       " 'inews.hket.com-2022-04.warc.gz',\n",
       " 'inews.hket.com-2022-05.warc.gz',\n",
       " 'inews.hket.com-2022-06.warc.gz',\n",
       " 'inews.hket.com-2023-03.warc.gz',\n",
       " 'inews.hket.com-2023-06.warc.gz',\n",
       " 'inews.hket.com-2023-07.warc.gz',\n",
       " 'news.ltn.com.tw-2016-08.warc.gz',\n",
       " 'news.ltn.com.tw-2016-09.warc.gz',\n",
       " 'news.ltn.com.tw-2016-12.warc.gz',\n",
       " 'news.ltn.com.tw-2017-02.warc.gz',\n",
       " 'news.ltn.com.tw-2017-03.warc.gz',\n",
       " 'news.ltn.com.tw-2017-04.warc.gz',\n",
       " 'news.ltn.com.tw-2022-06.warc.gz',\n",
       " 'news.ltn.com.tw-2023-04.warc.gz',\n",
       " 'news.ltn.com.tw-2023-06.warc.gz',\n",
       " 'news.ltn.com.tw-2023-07.warc.gz',\n",
       " 'news.mingpao.com-2019-09.warc.gz',\n",
       " 'news.mingpao.com-2022-04.warc.gz',\n",
       " 'news.mingpao.com-2022-06.warc.gz',\n",
       " 'news.mingpao.com-2023-04.warc.gz',\n",
       " 'news.mingpao.com-2023-06.warc.gz',\n",
       " 'news.mingpao.com-2023-07.warc.gz',\n",
       " 'news.sina.com.tw-2016-12.warc.gz',\n",
       " 'news.sina.com.tw-2017-01.warc.gz',\n",
       " 'news.sina.com.tw-2017-02.warc.gz',\n",
       " 'news.sina.com.tw-2017-03.warc.gz',\n",
       " 'news.sina.com.tw-2017-04.warc.gz',\n",
       " 'orientaldaily.on.cc-2023-03.warc.gz',\n",
       " 'orientaldaily.on.cc-2023-04.warc.gz',\n",
       " 'topick.hket.com-2022-04.warc.gz',\n",
       " 'topick.hket.com-2023-03.warc.gz',\n",
       " 'topick.hket.com-2023-04.warc.gz',\n",
       " 'topick.hket.com-2023-06.warc.gz',\n",
       " 'world.people.com.cn-2019-08.warc.gz',\n",
       " 'world.people.com.cn-2022-05.warc.gz',\n",
       " 'world.people.com.cn-2022-06.warc.gz',\n",
       " 'world.people.com.cn-2023-03.warc.gz',\n",
       " 'world.people.com.cn-2023-04.warc.gz',\n",
       " 'world.people.com.cn-2023-05.warc.gz',\n",
       " 'world.people.com.cn-2023-06.warc.gz',\n",
       " 'world.people.com.cn-2023-07.warc.gz',\n",
       " 'www.am730.com.hk-2022-06.warc.gz',\n",
       " 'www.am730.com.hk-2023-04.warc.gz',\n",
       " 'www.am730.com.hk-2023-05.warc.gz',\n",
       " 'www.am730.com.hk-2023-06.warc.gz',\n",
       " 'www.am730.com.hk-2023-07.warc.gz',\n",
       " 'www.chinatimes.com-2016-08.warc.gz',\n",
       " 'www.chinatimes.com-2016-09.warc.gz',\n",
       " 'www.chinatimes.com-2016-12.warc.gz',\n",
       " 'www.chinatimes.com-2017-01.warc.gz',\n",
       " 'www.chinatimes.com-2017-02.warc.gz',\n",
       " 'www.chinatimes.com-2017-03.warc.gz',\n",
       " 'www.chinatimes.com-2017-04.warc.gz',\n",
       " 'www.chinatimes.com-2019-08.warc.gz',\n",
       " 'www.chinatimes.com-2019-09.warc.gz',\n",
       " 'www.chinatimes.com-2022-04.warc.gz',\n",
       " 'www.chinatimes.com-2022-05.warc.gz',\n",
       " 'www.chinatimes.com-2022-06.warc.gz',\n",
       " 'www.chinatimes.com-2023-03.warc.gz',\n",
       " 'www.chinatimes.com-2023-04.warc.gz',\n",
       " 'www.chinatimes.com-2023-05.warc.gz',\n",
       " 'www.chinatimes.com-2023-06.warc.gz',\n",
       " 'www.chinatimes.com-2023-07.warc.gz',\n",
       " 'www.cna.com.tw-2017-01.warc.gz',\n",
       " 'www.cna.com.tw-2017-02.warc.gz',\n",
       " 'www.cna.com.tw-2019-08.warc.gz',\n",
       " 'www.cna.com.tw-2019-09.warc.gz',\n",
       " 'www.cna.com.tw-2022-04.warc.gz',\n",
       " 'www.cna.com.tw-2022-05.warc.gz',\n",
       " 'www.cna.com.tw-2022-06.warc.gz',\n",
       " 'www.cna.com.tw-2023-03.warc.gz',\n",
       " 'www.cna.com.tw-2023-04.warc.gz',\n",
       " 'www.cna.com.tw-2023-07.warc.gz',\n",
       " 'www.etnet.com.hk-2019-08.warc.gz',\n",
       " 'www.etnet.com.hk-2019-09.warc.gz',\n",
       " 'www.etnet.com.hk-2022-05.warc.gz',\n",
       " 'www.etnet.com.hk-2022-06.warc.gz',\n",
       " 'www.etnet.com.hk-2023-03.warc.gz',\n",
       " 'www.etnet.com.hk-2023-04.warc.gz',\n",
       " 'www.etnet.com.hk-2023-05.warc.gz',\n",
       " 'www.etnet.com.hk-2023-06.warc.gz',\n",
       " 'www.etnet.com.hk-2023-07.warc.gz',\n",
       " 'www.hk01.com-2019-08.warc.gz',\n",
       " 'www.hk01.com-2019-09.warc.gz',\n",
       " 'www.hk01.com-2022-05.warc.gz',\n",
       " 'www.hk01.com-2022-06.warc.gz',\n",
       " 'www.hk01.com-2023-04.warc.gz',\n",
       " 'www.hk01.com-2023-05.warc.gz',\n",
       " 'www.hk01.com-2023-06.warc.gz',\n",
       " 'www.hk01.com-2023-07.warc.gz',\n",
       " 'www.inform.kz-2023-03.warc.gz',\n",
       " 'www.inform.kz-2023-04.warc.gz',\n",
       " 'www.inform.kz-2023-05.warc.gz',\n",
       " 'www.inform.kz-2023-06.warc.gz',\n",
       " 'www.inform.kz-2023-07.warc.gz',\n",
       " 'www.ithome.com-2019-08.warc.gz',\n",
       " 'www.ithome.com-2019-09.warc.gz',\n",
       " 'www.ithome.com-2022-04.warc.gz',\n",
       " 'www.ithome.com-2022-06.warc.gz',\n",
       " 'www.ithome.com-2023-05.warc.gz',\n",
       " 'www.nownews.com-2019-09.warc.gz',\n",
       " 'www.nownews.com-2022-04.warc.gz',\n",
       " 'www.nownews.com-2022-05.warc.gz',\n",
       " 'www.nownews.com-2022-06.warc.gz',\n",
       " 'www.nownews.com-2023-03.warc.gz',\n",
       " 'www.nownews.com-2023-04.warc.gz',\n",
       " 'www.nownews.com-2023-06.warc.gz',\n",
       " 'www.rfi.fr-2022-04.warc.gz',\n",
       " 'www.rfi.fr-2022-05.warc.gz',\n",
       " 'www.rfi.fr-2022-06.warc.gz',\n",
       " 'www.rfi.fr-2023-03.warc.gz',\n",
       " 'www.rfi.fr-2023-04.warc.gz',\n",
       " 'www.rfi.fr-2023-05.warc.gz',\n",
       " 'www.rfi.fr-2023-06.warc.gz',\n",
       " 'www.rfi.fr-2023-07.warc.gz',\n",
       " 'www.taiwannews.com.tw-2017-01.warc.gz',\n",
       " 'www.taiwannews.com.tw-2017-02.warc.gz',\n",
       " 'www.taiwannews.com.tw-2017-04.warc.gz',\n",
       " 'www.taiwannews.com.tw-2019-08.warc.gz',\n",
       " 'www.taiwannews.com.tw-2019-09.warc.gz',\n",
       " 'www.taiwannews.com.tw-2022-05.warc.gz',\n",
       " 'www.taiwannews.com.tw-2023-03.warc.gz',\n",
       " 'www.thenewslens.com-2019-08.warc.gz',\n",
       " 'www.thenewslens.com-2022-04.warc.gz',\n",
       " 'www.zaobao.com.sg-2017-04.warc.gz',\n",
       " 'www.zaobao.com.sg-2019-09.warc.gz',\n",
       " 'www.zaobao.com.sg-2022-04.warc.gz',\n",
       " 'www.zaobao.com.sg-2022-05.warc.gz',\n",
       " 'www.zaobao.com.sg-2023-03.warc.gz',\n",
       " 'www.zaobao.com.sg-2023-04.warc.gz',\n",
       " 'www.zaobao.com.sg-2023-05.warc.gz',\n",
       " 'www.zaobao.com.sg-2023-06.warc.gz',\n",
       " 'www.zaobao.com.sg-2023-07.warc.gz',\n",
       " 'www2.hkej.com-2016-12.warc.gz',\n",
       " 'www2.hkej.com-2017-01.warc.gz',\n",
       " 'www2.hkej.com-2017-03.warc.gz',\n",
       " 'www2.hkej.com-2017-04.warc.gz',\n",
       " 'www2.hkej.com-2019-08.warc.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find pandas csv file with archieves we already compleated\n",
    "tracking_file = 'processed_files.csv'\n",
    "\n",
    "def try_to_find_progress_file(bucket, key = tracking_file):\n",
    "    processed_df = None\n",
    "    try:\n",
    "        get_processed_file = s3.get_object(Bucket=bucket, Key = key)\n",
    "        processed_df = pd.read_csv(io.BytesIO(get_processed_file['Body'].read()))\n",
    "        processed_files = set(processed_df['filename'])\n",
    "    except Exception as ex:\n",
    "        print(f\"Some struggle with processed files: {ex}\")\n",
    "        processed_files = set()\n",
    "    return processed_files\n",
    "\n",
    "processed_files = try_to_find_progress_file(bucket_name)\n",
    "\n",
    "# leave only newly uploaded files\n",
    "new_files = list(map(str, filter(lambda path: path not in processed_files, files)))\n",
    "\n",
    "new_files    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdcf25-5029-42bd-ae46-c9e6d4f46e68",
   "metadata": {},
   "source": [
    "# Archives Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79cfd433-3cd1-498c-98f8-06ec88775068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:10.537081Z",
     "iopub.status.busy": "2024-06-15T03:54:10.536505Z",
     "iopub.status.idle": "2024-06-15T03:54:11.145431Z",
     "shell.execute_reply": "2024-06-15T03:54:11.144762Z",
     "shell.execute_reply.started": "2024-06-15T03:54:10.537049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gzip\n",
    "import chardet\n",
    "\n",
    "from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "from fastwarc.stream_io import GZipStream, FileStream\n",
    "\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d241417-cba1-4054-8966-7663777c4091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:11.147191Z",
     "iopub.status.busy": "2024-06-15T03:54:11.146381Z",
     "iopub.status.idle": "2024-06-15T03:54:18.199980Z",
     "shell.execute_reply": "2024-06-15T03:54:18.199296Z",
     "shell.execute_reply.started": "2024-06-15T03:54:11.147163Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-06-15 03:54:12.767130: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-15 03:54:12.814431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 03:54:13.595570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name='DMetaSoul/sbert-chinese-general-v2'\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f315c3-4ba4-448e-a832-d2a9eb0f4af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T03:54:18.202423Z",
     "iopub.status.busy": "2024-06-15T03:54:18.201517Z",
     "iopub.status.idle": "2024-06-15T03:54:18.217897Z",
     "shell.execute_reply": "2024-06-15T03:54:18.217113Z",
     "shell.execute_reply.started": "2024-06-15T03:54:18.202387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_raw_content(encoding, raw_content):\n",
    "    encoding = encoding if encoding else chardet.detect(raw_content)['encoding']\n",
    "    html_content = raw_content.decode(encoding if encoding else 'utf-8', errors='ignore')\\\n",
    "    .encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def retrieve_main_text_info(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    paragraphs = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
    "    titles = \" \".join([t.get_text() for t in soup.find_all(['title', 'h1', 'h2'])])\n",
    "    return paragraphs.strip(), titles.strip()\n",
    "\n",
    "\n",
    "def extract_html_from_warc(warc_file_path):\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=warc_file_path)\n",
    "    server_stream = response\n",
    "    archieve_stream = GZipStream(io.BytesIO(server_stream['Body'].read()))\n",
    "    \n",
    "    for record in ArchiveIterator(archieve_stream):\n",
    "        if not record.headers.get('Content-Type') == 'application/http; msgtype=response' and \\\n",
    "              not 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "            continue\n",
    "        payload = record.reader.read()\n",
    "        \n",
    "        html_content = encode_raw_content(record.http_charset, payload)\n",
    "        year, month = record.record_date.year, record.record_date.month\n",
    "        content, title = retrieve_main_text_info(html_content)\n",
    "        yield Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'url': record.headers.get('WARC-Target-URI'),\n",
    "                    'year': year,\n",
    "                    'month': month,\n",
    "                    'title' : title\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b816f2-7382-471f-9e68-7d108320bd75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 33 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 39 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 45 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 39 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 46 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 43 documents to OpenSearch.\n",
      "Added 40 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 38 documents to OpenSearch.\n",
      "Added 42 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 39 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 41 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 22 documents to OpenSearch.\n",
      "Added 19 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 41 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 47 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 40 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 42 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 42 documents to OpenSearch.\n",
      "Added 38 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 24 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 24 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 41 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 35 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 24 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 40 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 24 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 34 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 39 documents to OpenSearch.\n",
      "Added 28 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 27 documents to OpenSearch.\n",
      "Added 24 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 36 documents to OpenSearch.\n",
      "Added 38 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 25 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 32 documents to OpenSearch.\n",
      "Added 22 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 31 documents to OpenSearch.\n",
      "Added 33 documents to OpenSearch.\n",
      "Added 37 documents to OpenSearch.\n",
      "Added 29 documents to OpenSearch.\n",
      "Added 26 documents to OpenSearch.\n",
      "Added 23 documents to OpenSearch.\n",
      "Added 20 documents to OpenSearch.\n",
      "Added 40 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n",
      "Added 30 documents to OpenSearch.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The embeddings count, 762 is more than the [bulk_size], 500. Increase the value of [bulk_size].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60944/3214077199.py\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m process_warc_files(\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mnew_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0membeddings_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_60944/3214077199.py\u001b[0m in \u001b[0;36mprocess_warc_files\u001b[0;34m(file_paths, embeddings_model, opensearch_url, http_auth, use_ssl, verify_certs, ca_certs, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mchunked_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mdocsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Added {len(chunked_documents)} documents to OpenSearch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/vectorstores.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     async def aadd_documents(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, bulk_size, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m    613\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         return self.__add(\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py\u001b[0m in \u001b[0;36m__add\u001b[0;34m(self, texts, embeddings, metadatas, ids, bulk_size, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     ) -> List[str]:\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0m_validate_embeddings_and_bulk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbulk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mindex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mtext_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_field\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py\u001b[0m in \u001b[0;36m_validate_embeddings_and_bulk_size\u001b[0;34m(embeddings_length, bulk_size)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embeddings size is zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbulk_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0membeddings_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;34mf\"The embeddings count, {embeddings_length} is more than the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;34mf\"[bulk_size], {bulk_size}. Increase the value of [bulk_size].\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The embeddings count, 762 is more than the [bulk_size], 500. Increase the value of [bulk_size]."
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text, stopwords_cn):\n",
    "    # Remove unwanted characters and normalize spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word not in stopwords_cn]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def process_documents(docs_transformed, text_splitter, stopwords_cn):\n",
    "    # Clean the text in the documents\n",
    "    cleaned_docs = [\n",
    "        Document(\n",
    "            page_content=clean_text(doc.page_content, stopwords_cn),\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        for doc in docs_transformed\n",
    "    ]\n",
    "\n",
    "    # Split the cleaned documents into chunks\n",
    "    chunked_documents = text_splitter.split_documents(cleaned_docs)\n",
    "    return chunked_documents\n",
    "\n",
    "def process_warc_files(file_paths, embeddings_model, opensearch_url, http_auth, use_ssl, verify_certs, ca_certs, batch_size=10):\n",
    "    # html2text_transformer = Html2TextTransformer()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )\n",
    "\n",
    "    # NLTK stopwords for Chinese\n",
    "    stopwords_cn = set(stopwords.words('chinese'))\n",
    "\n",
    "    # Initialize OpenSearch client with an empty list (temporarily)\n",
    "    docsearch = OpenSearchVectorSearch(\n",
    "        embedding_function=embeddings_model,\n",
    "        index_name = \"china-embeddings\",\n",
    "        opensearch_url=opensearch_url,\n",
    "        http_auth=http_auth,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs,\n",
    "        engine='faiss'\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for file_path in file_paths:\n",
    "            print(f\"Processing {file_path}...\")\n",
    "            docs_transformed = list(extract_html_from_warc(file_path))\n",
    "            print(f\"Processed {len(docs_transformed)} HTML documents\")\n",
    "            \n",
    "            if not docs_transformed:\n",
    "                print(f\"No documents found in {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Batch processing\n",
    "            for i in range(0, len(docs_transformed), batch_size):\n",
    "                batch = docs_transformed[i:i + batch_size]\n",
    "                futures.append(executor.submit(process_documents, batch, text_splitter, stopwords_cn))\n",
    "\n",
    "            for future in futures:\n",
    "                chunked_documents = future.result()\n",
    "                docsearch.add_documents(chunked_documents)\n",
    "                print(f\"Added {len(chunked_documents)} documents to OpenSearch.\")\n",
    "            futures = []\n",
    "\n",
    "    print(\"Processing and indexing completed.\")\n",
    "\n",
    "# Example usage\n",
    "process_warc_files(\n",
    "    new_files,\n",
    "    embeddings_model=embeddings_model,\n",
    "    opensearch_url=HOSTS[0],\n",
    "    http_auth=(\"admin\", PASS),\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    ca_certs=CA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1af1f-6765-467a-9528-8a5c4f4d2594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_processed_df = pd.DataFrame(new_files, columns=['filename'])\n",
    "if os.path.exists(tracking_file):\n",
    "    new_processed_df.to_csv(tracking_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    new_processed_df.to_csv(tracking_file, index=False)\n",
    "\n",
    "\n",
    "s3.upload_file(tracking_file, bucket_name, tracking_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bbff0-7009-4e2b-b4a3-e3914742e47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
